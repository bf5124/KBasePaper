{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score\n",
    "import os\n",
    "import gzip\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a string of length bytes long\n",
    "def dataloader(filepath, length):\n",
    "    f = gzip.GzipFile(fileobj=open(filepath, 'rb'))\n",
    "    data = f.read(length)\n",
    "    return data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the data into num_chunks\n",
    "def chunk(in_string,num_chunks):\n",
    "    chunk_size = len(in_string)//num_chunks\n",
    "    if len(in_string) % num_chunks: chunk_size += 1\n",
    "    iterator = iter(in_string)\n",
    "    for _ in range(num_chunks):\n",
    "        accumulator = list()\n",
    "        for _ in range(chunk_size):\n",
    "            try: accumulator.append(next(iterator))\n",
    "            except StopIteration: break\n",
    "        yield ''.join(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileClassifier(Dataset, ngram_range, max_features, param, num_round):\n",
    "    \n",
    "    X = Dataset.iloc[:,0]\n",
    "    y = Dataset.iloc[:,1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "    char_vectorizer = TfidfVectorizer(analyzer = 'char',\n",
    "        ngram_range = ngram_range, max_features = max_features)\n",
    "    char_vectorizer.fit(X)\n",
    "    train_chars = char_vectorizer.transform(X_train)\n",
    "    test_chars = char_vectorizer.transform(X_test)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(train_chars, label=y_train)\n",
    "    dtest = xgb.DMatrix(test_chars, label=y_test)\n",
    "    \n",
    "    model = xgb.train(param, dtrain, num_round)\n",
    "    preds = model.predict(dtest)\n",
    "    best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "    print(precision_score(y_test, best_preds, average='macro'))\n",
    "    print(np.asarray(y_test).reshape(1,-1))\n",
    "    \n",
    "    return(best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataloader('/Users/bf/Desktop/BNL2020/BioClassifierFiles/GCA_902387845.1_UHGG_MGYG-HGUT-02512_genomic.fna.gz', 40000)\n",
    "data1 = dataloader('/Users/bf/Desktop/BNL2020/BioClassifierFiles/SRR9259133.fastq.gz', 40000)\n",
    "data2 = dataloader('/Users/bf/Desktop/BNL2020/BioClassifierFiles/GCA_002097535.1_ASM209753v1_genomic.gff.gz', 40000)\n",
    "data3 = dataloader('/Users/bf/Desktop/BNL2020/BioClassifierFiles/GCA_003568845.1_ASM356884v1_genomic.gbff.gz', 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the 40000 byte data into 1000 byte pieces\n",
    "FastDat = list(chunk(data,40)) + list(chunk(data1, 40)) + list(chunk(data2, 40)) + list(chunk(data3, 40))\n",
    "data = {'FastDat': FastDat,\n",
    "        'Type': np.concatenate((np.repeat(0,40),np.repeat(1,40), np.repeat(2,40), np.repeat(3,40)))\n",
    "    }\n",
    "df = pd.DataFrame(data, columns = ['FastDat', 'Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 4,  # the maximum depth of each tree\n",
    "    'eta': 0.2,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 4}  # the number of classes that exist in this datset\n",
    "num_round = 40 \n",
    "ngram_range = (2,4)\n",
    "max_features = 10000\n",
    "Dataset = df\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:17:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "1.0\n",
      "[[2 2 3 1 2 0 2 1 2 3 0 2 0 1 0 0 3 3 2 0 1 0 0 0 0 3 2 1 3 2 3 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 1, 2, 0, 2, 1, 2, 3, 0, 2, 0, 1, 0, 0, 3, 3, 2, 0, 1, 0,\n",
       "       0, 0, 0, 3, 2, 1, 3, 2, 3, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileClassifier(Dataset, ngram_range, max_features, param, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
